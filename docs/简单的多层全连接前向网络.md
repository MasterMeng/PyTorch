# 简单的多层全连接前向网络  

深度学习的前身是神经网络，但由于之前的计算能力与数据量不足、传统机器学习方法如支持向量机完美的数学解释等原因，使得深度学习一直没有发展起来，直到近几年才有了爆发式的发展。接下来从神经网络算法入手来介绍深度学习。  

## 单层神经网络的分类器  

一个简单的一层神经网络的数学形式是特别简单的，可以看成一个线性运算再加上一个激活函数，正如一个神经元可以对一个输入进行不同的操作，可以是“喜欢”（激活变大）或者是“不喜欢”（激活变小），正式由于激活函数的作用，我们可以将一层神经网络用作分类器：正样本让激活函数激活变大，负样本让激活函数激活变小。  

例如我们可以使用Sigmoid激活函数做一个二分类问题，此时使用数学形式$\delta(\sum^n_{i=1}w_ix_i+b)$表示$P(y_i = 1|x_i,w)$，即输出位1类的概率，那么输出为0类的概率为$1-P(y_i = 1|x_i,w)$,将$P(y_i = 1|x_i,w)$展开可得：  

$$P(y_i = 1|x_i,w) = \delta(\sum^n_{i=1}w_ix_i+b)$$
$$ = \frac{1}{1+e^{-(w_ix_i+b)}}$$
$$ = \frac{e^{w_ix_i+b}}{1+e^{w_ix_i+b}}$$  

这就是前面介绍过的Logistic回归，所以Logistic回归只是一个是用来Sigmoid作为激活函数的一个层神经网络。  

## 激活函数  

下面介绍一下神经网络中的各种常用的激活函数。  

### Sigmoid  

Sigmoid非线性激活函数的数学表达式是$\delta(x) = \frac{1}{1+e^{-1}}$，其图形如下图所示。从图像可知，Sigmoid激活函数试讲一个实数输入转化到0 ~ 1之间的输出，具体来说就是将越小的负数转化到越靠近0，越大的正数转化到越靠近1。

![Sigmoid函数图像](../pics/Sigmoid函数图像.png)  

#### 优势  

引其具有良好的解释性，历史上被频繁使用。  

#### 缺点  

最近几年，Sigmoid激活函数已经越来越少地被人使用，主要是因为以下两大缺点：  

1. Sigmoid函数会造成梯度消失 。最大的特点就是Sigmoid函数在靠近1和0的两端时，梯度会几乎变成0。梯度下降法通过梯度乘上学习率来更新参数，因此如果梯度接近0，那么将没有任何信息来更新参数，这样就会造成模型不能收敛。另外，如果使用Sigmoid函数，那么需要在初始化权重的时候必须非常小心。如果初始化的时候权重太大，那么经过激活函数也会导致大多数神经元变得饱和，无法更新参数。  
 
2. Sigmoid输出不是以0为均值，这就导致进过Sigmoid激活函数之后的输出，作为后面一层网络的输入的时候是非0均值，此时如果进入下一次神经元的输入全是正的，将导致梯度全是正的，那么在更新参数的时候都是正梯度。不过因为一般神经网络在训练的时候都是按batch进行训练的，可以在一定程度上缓解这个问题。  

### Tanh  

Tanh激活函数是上面Sigmoid激活函数的变形，其数学表达式为$tanh(x) = 2\delta(2x) - 1$，图形如下所示：  

![Tanh函数图形](../pics/Tanh函数图形.png)  

它将输入的数据转化到-1 ~ 1之间，从图像可以看出它将输出变成了0均值，在一定程度上解决了Sigmoid函数的第二个问题，但仍存在梯度消失的问题。相比Sigmoid函数，Tanh激活函数更具优势。  

### ReLU