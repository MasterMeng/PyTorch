# 卷积神经网络的原理和结构  

在介绍卷积神经网络之前，需要明确三个观点，正是这三个观点使得卷积神经网络能够真正起作用。  

1. 局部性

对于一张图片而言，需要检测图片中的特征来决定图片的类别，通常情况下这些特征都不是由整张图片决定的，而是由一些局部的区域决定的。  

2. 相同性

对于不同的图片，如果它们具有同样的特征，这些特征会出现在图片的不同位置，也就是说可以用同样的检测模式去检测不同图片的相同特征，只不过这些特征处于图片的不同位置，但是特征检测所做的操作几乎一样。  

3. 不变性

对于一张大图片，如果我们进行采样，那么图片的性质基本保持不变。  

以上的三个性质分别对应着卷积神经网络中的三种思想，接下来介绍网络的层结构。  

卷积神经网络和全连接神经网络相似，也是由一些神经元构成的。这些神经元中有着需要学习的参数，通过网络输入，最后输出结果，通过损失函数来优化网络中的参数。卷积神经网络的不同之处在于网络的层结构是不同的。全连接神经网络由一系列隐藏层构成，每个隐藏层由若干个神经元构成，其中每个神经元都与前一层的所有的神经元相关联，但是每一层中的神经元是相互独立的。卷积神经网络是一个3D容量的神经元，也就是说神经元是以三个维度来排列的：宽度、高度和深度。  

卷积神经网络中的主要层结构有三个：卷积层、池化层和全连接层，通过堆叠这些层结构形成了一个完整的卷积神经网络结构。卷积神经网络将原始图片转化成最后的类别得分，其中一些层包含参数，一些曾没有包含参数，比如卷积层和全连接层拥有参数，而激活层和池化层不含参数。这些参数通过梯度下降法来更新，最后使得模型尽可能正确地识别出图片类别。  

接下来具体介绍每一种层的连接方式和它们的超参数。  

## 卷积层  

卷积层是卷积神经网络的核心，大多数计算都是在卷积层中进行的。  

1. 概述  

首先介绍卷积神经网络的参数。这些参数是由一些可学习的滤波器集合构成的，每个滤波器在空间上都比较小，但是深度和输入数据的深度保持一致。当滤波器沿着输入数据的宽度和高度滑动时，会生成一个二维的激活区，激活图上的每个空间位置表示的原图片对于该滤波器的反应。  

在每个卷积层上，会有一整个集合的滤波器，比如20个，这样就会形成20张二维的、不同的激活图，将这些激活图在深度方向上层叠起来就形成了卷积层的输出。  

如果用大脑和生物神经元作比喻，那么输出的3D数据中的每个数据都可以看成是神经元的一个输出，而该神经元只是观察输入数据中的一特征，并且和空间上左右两边的所有神经元共享参数（因为这些输出都是使用同一个滤波器得到的结果）。下面介绍卷积神经网络中的神经元连接，它们在空间中的排列已经它们参数共享的模式。  

2. 局部连接

在处理图像这样高纬度输入的时候，让每个神经元都与它那一层中的所有神经元进行全连接是不现实的。相反，让每个神经元只与输入数据的一个局部区域连接是可行的，因为图片特征的局部性，所以只需要通过局部就能取出相应的特征。  

与神经元连接的空间大小叫做神经元的感受野（receptive field），它的大小是一个人为设置的超参数，其实就是滤波器的宽好高。在深度方向上，其大小总是和输入的深度相等。对待空间维度（宽和高）和深度维度是不同的，连接在空间上是局部的，但在深度上总是和输入的数据深度保持一致。  

3. 空间排列  

前面介绍了每个神经元只需要与输入数据的局部区域相连接，但是没有介绍卷积层中神经元的数量和它们的排列方式、输出深度、滑动步长，以及边界填充控制着卷积层的空间排布。  

首先，卷积层的输出深度是一个超参数，它与使用的滤波器数量一致，每一种滤波器所做的就是在输入数据中寻找一种特征。  

其次，在滑动滤波器的时候，必须指定步长。滑动的操作会使得输出的数据在空间上变得更小。  

最后介绍边界填充，可以将输入数据用0在边界进行填充，这里将0填充的尺寸作为一个超参数，它的一个好处就是可以控制输出数据在空间上的尺寸，常用来保证输入和输出在空间上尺寸一致。  

输出的尺寸多少可以用公式$\frac{W - F +2P}{S} + 1$，其中W表示输入的数据大小，F表示卷积层中神经元的感受野尺寸，S表示步长，P表示边界填充0的数量。  

4. 零填充的使用  

一般来说，当步长$S = 1$时，零填充的值为$P = \frac{F - 1}{2}$，这样就能够保证输入的数据和输出的数据具有相同的空间尺寸。  

5. 步长的限制  

通过上面的公式可以看成步长的选择是有所限制的。举例来说，当输入尺寸$W = 10$的时候，如果不使用零填充，即$P = 0$，滤波器尺寸$F = 3$，这时步长$S = 2$就行不通，因为$\frac{10 - 3 + 0}{2} + 1 = 4.5$，结果不是一个整数，这就说明神经元不能整齐对称地滑过输入数据体，这样的超参数设置是无效的。此时可以使用零填充让设置变得合理。  

6. 参数共享  

在卷积层使用参数共享可以有效地减少参数的个数，之所以能够行得通是因为之前介绍的特征的相同性，也就是说相同的滤波器能够检测出不同位置的相同特征。  

7. 总结  

最后总结下卷积层的一些性质。  

- 输入数据体的尺寸是$W_1$ x $H_1$ x $D_1$。
- 4个超参数：滤波器数量$K$，滤波器空间尺寸$F$，滑动步长$S$，零填充的数量$P$。
- 输出数据体的尺寸为$W_2$ x $H_2$ x $D_2$，其中$W_2 = \frac{W_1 - F + 2P}{S} +1,H_2 = \frac{H_1 - F + 2P}{S} + 1,D_2 = K$。
- 由于参数共享，每个滤波器包含的权重数目为$F$ x $F$ x $D_1$，卷积层一共有$F$ x $F$ x $D_1$ x $K$个权重和$K$个偏置。
- 在输出体数据中，第$d$个深度切片（空间尺寸为$W_2$ x $H_2$），是第$d$个滤波器和输入数据进行有效卷积运算的结果，再加上第$d$个偏置。  

## 池化层  

通常会在卷积层之间周期性插入一个池化层，起作用是逐渐降低数据体的空间尺寸，这样就能够减少网络中的参数数量，减少计算资源耗费，同时也能够有效地控制过拟合。  

池化层和卷积层一样，也有一个空间窗口，通常采用的是取这些窗口中的最大值作为输出结果，然后不断滑动窗口，对输入数据体每个深度切片单独处理，减少它的空间尺寸。  

池化层之所以有效，是因为之前介绍的图片特征具有不变性，即通过下采样不会丢失图片用于的特征。正是因为这种特征，我们可以将图片缩小在进行卷积处理，这样可以大大降低卷积运算的时间。  

池化层有一些和卷积层类似的性质：  

- 输入数据体的尺寸是$W_1$ x $H_1$ x $D_1$。
- 两个超参数：空间大小$F$，滑动步长$S$。
- 输出数据体的尺寸为$W_2$ x $H_2$ x $D_2$，其中$W_2 = \frac{W_1 - F}{S} +1,H_2 = \frac{H_1 - F}{S} + 1,D_2 = D_1$。
- 对输入进行固定函数的计算，没有参数引入。
- 很少引入零填充。  

## 全连接层  

全连接层和之前介绍的一般的神经网络的结构是一样的，每个神经元与前一层所有的神经元全部连接，而卷积神经网络只和输入数据中的一个局部区域连接，并且输出的神经元每个深度切片共享参数。

一般经过了一系列的卷积层和池化层之后，提取出图片的特征图，然后将特征图中的所有神经元变成全连接层的样子，再经过几个隐藏层，最后输出结果。在这个过程中为了防止过拟合会引入Dropout。最近研究表明，在进入全连接层之前，使用全局平均池化能够有效地降低过拟合。  

## 卷积神经网络的基本形式  

卷积神经网络中通常是由上面介绍的三种层结构所构成的，之前还介绍过引入激活函数增加模型的非线性，所以卷积神经网络最常见的形式就是将一些卷积层和ReLU层放在一起，有可能在ReLU层前面加上批标准化层，随后紧跟着池化层，再不断重复，直到图像在空间上班缩小到一个足够小的尺寸，然后将特征图展开，连接几层全连接层，最后输出结果，比如分类评分等。  

1. 小滤波器的有效性  

一般而言，几个小滤波器卷积层的组合比一个大滤波器卷积层要好。  

2. 网络的尺寸  

对于卷积神经网络的尺寸设计，没有严格的数学证明，这是根据经验指定出来的规则。  

* 输入层：一般而言，输入层的大小应该能够被2整除很多次，常用的数字包括32，64，96和224。
* 卷积层：卷积层应该尽可能使用小尺寸的滤波器，滑动步长取1。还有就是需要对输入数据体进行零填充，这样可以有效地保证卷积层不会改变输入数据体的空间尺寸。如果必须要使用更大的滤波器尺寸，通常用在第一个面对原始图像的卷积层上。  
* 池化层：池化层负责对输入的数据空间维度进行下采样，常用的设置使用2x2的感受野做最大值池化，滑动步长取2.另外一个不常用的设置是使用3x3的感受野，步长设置为2。一般而言池化层的感受野大小很少超过3，因为这样会使得池化过程过于激烈，造成信息的丢失，这通常会造成算法的性能变差。
* 零填充：零填充的使用可以让卷积层的输入和输出在空间上的维度保持一致。除此之外，如果不使用零填充，那么数据体的尺寸就会略微减少，在不断进行卷积的过程中，图像的边缘信息会过快地损失掉。
