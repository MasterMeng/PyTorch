# 处理数据和训练模型的技巧  

在开始训练网络之前，良好的数据预处理和参数初始胡能够达到事半功倍的效果。在模型训练中采用一些训练技巧，能够使得模型最后达到state-of-art的效果。本文主要介绍处理数据和训练模型的技巧。  

## 数据预处理  

1. 中心化

数据预处理中一个最常见的处理办法就是每个特征维度都减去相应的均值实现中心化，这样可以使得数据变成0均值，特别对于一些图像数据，为了方便我们将所有的数据都减去一个相同的值。  

2. 标准化

在使得数据都变成0的均值之后，还需要使用标准化的做法让数据不同的特征维度都有着相同的规模。有两种常用的方法：一种是除以标准差，这样可以使得新数据的分布接近标准高斯分布；还有一种做法就是让每个特征维度的最大值和最小值按比例缩放当-1 ~ 1 之间。  

如果知道输入不同特征有着不同的规模，那就需要使用标准化的做法来让他们处于同一个规模下，这对机器学习算法而言是非常重要的。

3. PCA  

PCA是另外一种处理数据的方法。在进行这一步以前，首先会将数据中心化，然后计算数据的协方差矩阵，这一步特别简单。假设输入是X = N x D，那么通过$\frac{X^TX}{N}$能够得到这个协方差矩阵。这个协方差矩阵是对称半正定的，可以通过这个协方差矩阵来进行奇异值分解（SVD），然后对数据进行去相关，将其投影到一个特征空间，我们可以取一些较大的、主要的特征向量来降低数据的维数，去掉一些没有方差的维度，即主成分分析（PCA）。  

这个操作对于一些线性模型和神经网络，都能取得良好的效果。

4. 白噪声

白噪声也是一种数据处理的方法，首先会跟PCA一样将数据投影到一个特征空间，然后每个维度除以特征值来标准化这些数据，直观上就是一个多元高斯分布转化到了一个0均值、协方差矩阵为1的多元高斯分布。  

在实际处理数据中，中心化和标准化都特别重要。我们计算训练集的统计量比如均值，然后将这些统计量应用到测试集合验证集当中。但是PCA和白噪声在卷积网络中基本不使用，因为卷积网络可以自动学习如何提取这些特征而不需要人工再去对其进行干预。  

## 权重初始化  

上面介绍了数据的预处理，在进入网络训练之前，需要做参数的预处理。下面介绍预处理的策略。  

1. 全0初始化  
   
首先从一个最直观，但是不应该采用的策略入手，即将参数全部初始化为0。  

为什么不能采用这一策略？首先，我们并不知道训练之后的网络最后权重更新的是多少，但是知道数据在进入网络之前进过了合适的预处理，所以我们可以假设最后的权重有一半是正的，一半是负的。那么将参数全部初始化为0似乎是一个非常好的选择，但是这样是不对的。因为如果神经网络中每个权重都被初始化成相同的值，那么每个神经元就会计算出相同的结果，在反向传播的时候也会计算出相同的梯度，最后导致所有权重都会有相同的更新。换句话说，如果每个权重都被初始化成相同的值，那么权重之间失去了不对称性。  

2. 随机初始化
   
我们希望权重初始化的时候能够尽量靠近0，但是并不能全都等于0，所以可以初始化权重为一些靠近0的随机数，通过这个方式可以打破对称性。这里面的核心想法就是神经元最开始都是随机的、唯一的，所以在更新的时候也是作为独立的部分，最后一起合成在神经网络当中。  

一般的随机化策略有高斯随机化、均匀随机化等，需要注意的是并不是越小的随机化产生的结果越好：因为权重初始化越小，方向传播中关于权重的梯度也越小，这是因为梯度与参数的大小是成比例的，所以这会极大地减弱梯度流的信号，成为神经网络的一个隐患。  

这个初始化策略还存在一个问题，那就是网络输出分布的方差会随着输入维度的增加而增大，这可以用下面的数学表达式来证明：  

$$Var(s) = Var(\sum^n_{i=1}w_ix_i)$$
$$ = \sum^n_{i=1}Var(w_ix_i)$$
$$\sum^n_{i=1}[E(w_i)]^2Var(x_i) + E[(x_i)]^2Var(w_i) + Var(x_i)Var(w_i)$$
$$\sum^n_{i=1}Var(x_i)Var(w_i)$$
$$(nVar(w))Var(x)$$  

其中假设输入和权重都是0均值的，也就是说$E[x_i] = E[w_i] = 0$，但这并不是一般的情况，比如进过了ReLU激活函数之后输出就会是一个正的均值，这里做这样的假设是为了方便计算。我们可以看到输出的结果$S$比输入$x$的方差增大了$nVar(w)$倍，如果网络越来越深，就会导致方差越来越大，所以希望$nVar(w)$尽可能接近1，也就是$nVar(w) = 1$，这样得到$nVar(w) = \frac{1}{n}$，也就是$w$初始胡之后需要除以$\sqrt{n}$  

3. 稀疏初始化

另外一种初始化的方法就是稀疏初始化，将权重全部初始化为0，然后为了打破对称性在里面随机挑选一些参数附上一些随机值。这种方法的好处是参数占用的内存较少，因为里面有较多的0，但实际中使用较少。  

4. 初始化偏置（bias）

对于偏置，通常是初始化为0，因为权重已经打破了对称性，所以使用0来初始化是简单的。  

5. 批标准化（Batch Normalization）

皮标准化是最近兴起的一项技术，它的核心想法就是标准化这个过程是可微的，减少了很多不合理初始化的问题，所以我们可以将标准化过程应用到神经网络的每一层中做前向传播和反向传播，通常皮标准化应用在全连接层后面、非线性层前面。  

实际中批标准化已经变成了神经网络中的标准技术，特别是在卷积神经网络中，他对于很坏的初始化有很强的鲁棒性，同时还可以加快网络的收敛速度。另外，批标准化还可以理解为在网络的每一层前面都会做数据的预处理。  

## 防止过拟合  

网络容量过大的话会造成过拟合，但是防止过拟合的最佳办法并不是减少网络容量，那么如何防止过拟合？接下来介绍几个具体的办法来防止过拟合。  

1. 正则化  

L2正则化是正则化（regularization）中比较常用的形式，它的想法是对于权重过大的部分进行惩罚，也就是直接在损失函数中增加权重的二犯数量级，也就是$\frac{1}{2}\lambda w^2$，其中$\lambda$是正则强度，通常使用0.5，因为对于$w^2$的梯度$2w$，使用$\frac{1}{2}$就能使得梯度是$\lambda w$而不是$2\lambda w$。所以使用L2正则化可以看成是权重更新在原来的基础上再$ - \lambda w$，这样可以让参数更新之后更加靠近0。  

L1正则化是另一种正则化方法，是在损失函数中增加权重的1范数，也就是$\lambda |w|$，我们也可以把L1正则化和L2正则化结合起来，如$\lambda_1|w| + \lambda_2 w^2$。L1正则化相对于L2正则化的优势是在优化的过程中可以让权重变得更加稀疏，换言之，在优化结束的时候，权重只会取一些与重要的输入有关的权重，这就使得与噪声相关的权重被尽可能将为0。L2正则化的优势在于最终的效果会比L1正则化更加发散，权重也会被限制得更小。  

除此之外，还有一种正则化方法叫做最大范数限制，其迫使权重在更新的过程中范数有个上界，即$||w|| < c$，这种办法可以使得学习率设置太高的时候网络不会“爆炸”，因为更新总是有界的。  

在实际中对于正则化的选择通常使用L2正则化。  

2. Dropout  

一种非常有效、简单、同时也是现在深度学习使用最为广泛的防止过拟合的方法。其核心想法就是在训练网络的时候依概率$P$保留每个神经元，也就是说每次训练的时候有些神经元会被设置为0。
   
可以把Dropout看做是集成的学习方法，每次训练Dropout之后，就可以看作是一个新的模型，然后训练了很多次之后就可以看成是这些模型的集成。  

以上介绍了多种防止过拟合的办法，在实际应用中，一般采用全局权重的L2正则化搭配Dropout来防止过拟合。
   