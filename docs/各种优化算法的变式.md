# 各种优化算法的变式  

在前面讲torch.optim的部分时，简单介绍了梯度下降法，接下来会先介绍下降法的原理，再详细地介绍各种基于梯度的优化方法。  

## 梯度下降法  

首先，梯度下降法的更新公式如下：  

$$\theta^i = \theta^{i-1} - \eta\nabla L(\theta^{i-1})$$  

现在希望通过实际理论来证明这样更新参数能够达到最优的效果。  

重新表述一下问题，希望求解下面的这个方程：  

$$\theta^* = arg_\theta minL(\theta)$$  

也就是说，更新参数之后要有下面的结果：  

$$L(\theta_0) > L(\theta_1) > L(\theta_2) > \cdots$$  

在解决这个问题之前，我们需要了解一下泰勒级数（Taylor Series）。  

对于任何一个无限可微函数$h(x)$，在一个点$x = x_0$附近，有以下的泰勒级数：  

$$h(x) = \sum^\infty_{k = 0}\frac{h^{(k)}(x_0)}{k!}(x - x_0)^k$$  
$$ = h(x_0) + h'(x_0)(x - x_0) + \frac{h''(x_0)}{2!}(x - x_0)^2 + \cdots$$  

当x足够接近$x_0$，有以下的近似：  

$$h(x) \approx h(x_0) + h'(x_0)(x-x_0)$$  

对于多元泰勒级数，有以下的公式：  

$$h(x,y) = h(x_0,y_0) + \frac{\partial h(x_0,y_0)}{\partial x}(x - x_0) + \frac{\partial h(x_0,y_0)}{\partial y}(y - y_0) + \cdots$$  

同样当x和y足够接近$x_0$和$y_0$的时候，有以下的近似：  

$$h(x,y) \approx h(x_0,y_0) +  \frac{\partial h(x_0,y_0)}{\partial x}(x - x_0) + \frac{\partial h(x_0,y_0)}{\partial y}(y - y_0)$$  

现在假设参数$\theta$有两个$\theta_1,\theta_2$，那么由上面的二元泰勒级数知道，对于一个点$(a,b)$，对于一个足够小的范围，有以下近似：  

$$L(\theta) = L(a,b) + \frac{\partial L(a,b)}{\partial \theta_1}(\theta_1 - a) + \frac{\partial L(a,b)}{\partial \theta_2}(\theta_2 - b)$$  

令$s = L(a,b),\mu = \frac{\partial L(a,b)}{\partial \theta_1},\nu = \frac{\partial L(a,b)}{\partial \theta_2}$，那么$L(\theta)$有以下的简单表达：  

$$L(\theta) \approx s + \mu(\theta_1 - a) + \nu(\theta_2 - b) $$  

我们知道$s,\mu,\nu$都是常数，所以希望找到$\theta_1,\theta_2$使得$L(\theta)$最小，同时$\theta_1,\theta_2$都是在$(a,b)$的一个小范围之内，所以要满足以下条件：  

$$(\theta_1 - a)^2 + (\theta_2 - b)^2 \leq \epsilon^2$$  

再换元，将$(\theta_1 - a) = \partial \theta_1,(\theta_2 - b) = \partial \theta_2$，同时由于$s$是一个与$\theta_1,\theta_2$没有关系的常量，所以可以去掉它，这样就能得到下面的式子：  

$$L(\theta) \Leftrightarrow \mu \partial \theta_1 + \nu \partial \theta_2$$  
$$\partial \theta^2_1 + \partial \theta^2_2 \leq \epsilon^2$$  

对于上面的这个式子，可以将$(\mu,\nu)$看做一个向量，同时$(\partial \theta_1,\partial \theta_2)$也是一个向量，那么$(\mu,\nu) \cdot (\partial \theta_1,\partial \theta_2) = \mu \partial \theta_1 + \nu \partial \theta_2$。那么怎么能够求到这个内积的最小值呢？很简单，只需要保证$(\partial \theta_1,\partial \theta_2)$是$(\mu,\nu)$的反方向，也就是$(\partial \theta_1,\partial \theta_2) = -(\mu,\nu)$即可，但是由于需要把$\partial \theta_1,\partial \theta_2$都限制在一个小的范围内，所以要对其范围做一个限制，也就是：  

$$(\partial \theta_1,\partial \theta_2) = -(\mu,\nu)$$  
$$\Leftarrow (\theta_1,\theta_2) = (a,b) - \eta(\mu,\nu)$$  

所以只要在一个足够晓得范围内，更新参数之后就能取得一个更小的值，更新公式如下：  

$$(\theta_1,\theta_2) = (a,b) - \eta(\frac{\partial L(a,b)}{\partial \theta_1},\frac{\partial L(a,b)}{\partial \theta_2})$$  

所以只要取得一个特别小的学习率$\eta$，就能够保证$\theta_1,\theta_2$在一个足够靠近$(a,b)$的范围内，这就实现了梯度下降法。  

接下来介绍一些常见的梯度下降法的变式。  

## 梯度下降法的变式  

1. SGD  

随机梯度下降法（SGD）是梯度下降法的一个小变形，就是每次使用一批（batch）数据进行梯度计算，而不是计算全部数据的梯度。因为现在深度学习的数据量都特别大，所以每次都计算所有数据的梯度是不现实的，这样会导致运算时间特别长，同时每次都计算全部的梯度还丢失了一些随机性，容易陷入局部误差，所以使用随机梯度下降法可能每次都不是朝着真正最小的方向，但这样反而容易跳出局部极小点。  

2. Momentum

第二种优化方法就是在随机梯度下降的同时，增加动量（Momentum）。这来自于物理中的概念，可以假设损失函数是一个山谷，一个球从山谷滑下来，在一个平坦的地势，球的滑动速度就会慢下来，可能陷入一些鞍点或者局部极小值。这个时候给它增加动量就可以让它从高处滑落时的势能转为平地的动能，相当于惯性增加了小球在平地的滑动速度，从而帮助其跳出鞍点或者局部极小值。  

那么动量怎么计算呢？动量的计算基于前面的梯度，也就是说参数更新不仅仅基于当前的梯度，也基于之前的梯度。  

除此之外，对于动量还有一个变形，即Nesterov。我们在更新参数的时候需要计算梯度，传统的动量方法是计算当前位置的梯度，但Nesterov的方法计算经过动量更新之后的位置的梯度。

3. Adagrad

这是一种自适应学习率（adaptive）的方法，它的公式是：  

$$w^{t+1} \leftarrow w^t - \frac{\eta}{\sqrt{\sum^t_{i=0}(g^i)^2} + \epsilon}g^t$$  

从上面的公式可以看出，学习率在不断变小，且受每次计算出来的梯度影响，对于梯度较大的参数，它的学习率就会变得相对更小，里面的根号特别重要，没有这个根号算法表现非常差。同时$\epsilon$是一个平滑参数，通常设置为$10^{-4}$ ~ $10^{-8}$，这是为了避免分母为0。  

自适应学习率的缺点就是在某些情况下一直递减的学习率并不好，这样会造成学习过早停止。

4. RMSprop

这是一种非常有效的自适应学习率的改进方法，它的公式如下：  

$$cache^t = \alpha * cache^{t-1} + (1 - \alpha)(g^t)^2$$
$$w^{t+1} \leftarrow w^t - \frac{\eta}{\sqrt{cache^t} + \epsilon}g^t$$  

这里多了一个$\alpha$，这是一个衰减率，也就是说RMSprop不再会将前面所有的梯度的平方求和，而是通过一个衰减率将其变小，使用了一种滑动平均的方式，越靠前面的梯度对自适应的学习率影响越小，这样就能更加有效地避免Adagrad学习率一直递减太多的问题，能够更快地收敛。

5. Adam

这是一种综合性的学习方法，可以看成是RMSprop加上动量的学习方法，达到比RMSprop更好的效果。  

以上介绍了多种基于梯度的参数更新方法，实际中我们可以使用Adam作为默认的优化算法，往往能够达到比较好的效果，同时SGD+Momentum的方法也值得尝试。
   